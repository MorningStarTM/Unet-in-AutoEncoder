{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/MorningStarTM/large-language-model-creation.git","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:02:44.713092Z","iopub.execute_input":"2024-07-24T15:02:44.713818Z","iopub.status.idle":"2024-07-24T15:02:46.243961Z","shell.execute_reply.started":"2024-07-24T15:02:44.713786Z","shell.execute_reply":"2024-07-24T15:02:46.242842Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'large-language-model-creation'...\nremote: Enumerating objects: 41, done.\u001b[K\nremote: Counting objects: 100% (41/41), done.\u001b[K\nremote: Compressing objects: 100% (25/25), done.\u001b[K\nremote: Total 41 (delta 11), reused 35 (delta 9), pack-reused 0\u001b[K\nUnpacking objects: 100% (41/41), 9.68 KiB | 901.00 KiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/large-language-model-creation","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:02:46.246020Z","iopub.execute_input":"2024-07-24T15:02:46.246338Z","iopub.status.idle":"2024-07-24T15:02:46.253015Z","shell.execute_reply.started":"2024-07-24T15:02:46.246310Z","shell.execute_reply":"2024-07-24T15:02:46.252080Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/large-language-model-creation\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:02:46.254196Z","iopub.execute_input":"2024-07-24T15:02:46.254505Z","iopub.status.idle":"2024-07-24T15:02:47.245517Z","shell.execute_reply.started":"2024-07-24T15:02:46.254479Z","shell.execute_reply":"2024-07-24T15:02:47.244346Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"LICENSE  README.md  models  notebooks  tokenizer\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nimport json\nfrom models import GPTLanguageModel\nfrom tokenizer import WordLevelTokenizer\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:02:47.248048Z","iopub.execute_input":"2024-07-24T15:02:47.248368Z","iopub.status.idle":"2024-07-24T15:02:50.527082Z","shell.execute_reply.started":"2024-07-24T15:02:47.248342Z","shell.execute_reply":"2024-07-24T15:02:50.526054Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\nmax_iters = 60000\nlearning_rate = 3e-4\nblock_size = 8\nbatch_size = 4\neval_iters = 500\nn_emb = 384\nn_layers = 4\nn_head = 4","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:10:39.285878Z","iopub.execute_input":"2024-07-24T15:10:39.286930Z","iopub.status.idle":"2024-07-24T15:10:39.292218Z","shell.execute_reply.started":"2024-07-24T15:10:39.286893Z","shell.execute_reply":"2024-07-24T15:10:39.291255Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# function for get vocab size","metadata":{}},{"cell_type":"code","source":"def get_vocab_size(corpus):\n    \"\"\"\n    Get the vocabulary size of the given corpus.\n\n    Parameters:\n    corpus (str): The text corpus to analyze.\n\n    Returns:\n    int: The size of the vocabulary (number of unique words and punctuation).\n    \"\"\"\n    words = preprocess_text(corpus)\n    unique_words = set(words)\n    return len(unique_words)\n\ndef preprocess_text(text):\n    \"\"\"\n    Preprocess the text by converting to lowercase and splitting into words and punctuation.\n\n    Parameters:\n    text (str): The text to preprocess.\n\n    Returns:\n    list: A list of words and punctuation.\n    \"\"\"\n    text = text.lower()\n    words = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text)\n    return words","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:02:50.536121Z","iopub.execute_input":"2024-07-24T15:02:50.536521Z","iopub.status.idle":"2024-07-24T15:02:50.544923Z","shell.execute_reply.started":"2024-07-24T15:02:50.536486Z","shell.execute_reply":"2024-07-24T15:02:50.544091Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# function for read json file","metadata":{}},{"cell_type":"code","source":"def read_json_files(directory_path):\n    all_text = \"\"\n\n    # Get the list of files in the directory\n    files = os.listdir(directory_path)\n\n    # Loop through the first n files in the directory\n    for filename in files[:1]:\n        if filename.endswith(\".json\"):\n            file_path = os.path.join(directory_path, filename)\n\n            # Read the JSON file\n            with open(file_path, 'r', encoding='utf-8') as file:\n                data = json.load(file)\n\n                # Iterate through each object in the array and concatenate the 'text' values\n                for item in data:\n                    if 'text' in item:\n                        all_text += item['text'] + \" \"\n\n    return all_text","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:02:50.546089Z","iopub.execute_input":"2024-07-24T15:02:50.546809Z","iopub.status.idle":"2024-07-24T15:02:50.555561Z","shell.execute_reply.started":"2024-07-24T15:02:50.546754Z","shell.execute_reply":"2024-07-24T15:02:50.554833Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Read data","metadata":{}},{"cell_type":"code","source":"text = read_json_files(\"/kaggle/input/plain-text-wikipedia-202011/enwiki20201020\") ","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:02:50.556721Z","iopub.execute_input":"2024-07-24T15:02:50.557084Z","iopub.status.idle":"2024-07-24T15:02:52.127479Z","shell.execute_reply.started":"2024-07-24T15:02:50.557053Z","shell.execute_reply":"2024-07-24T15:02:52.126444Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"text[0:500]","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:02:52.128812Z","iopub.execute_input":"2024-07-24T15:02:52.129104Z","iopub.status.idle":"2024-07-24T15:02:52.135958Z","shell.execute_reply.started":"2024-07-24T15:02:52.129079Z","shell.execute_reply":"2024-07-24T15:02:52.134967Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"\"Travis are a Scottish rock band formed in Glasgow in 1990, composed of Fran Healy (lead vocals, rhythm guitar), Dougie Payne (bass guitar, backing vocals), Andy Dunlop (lead guitar, banjo, backing vocals) and Neil Primrose (drums, percussion). The band's name comes from the Harry Dean Stanton character Travis Henderson from the film Paris, Texas. The band released their debut album, Good Feeling (1997), to moderate success where it debuted at number nine on the UK Albums Chart and went onto achi\""},"metadata":{}}]},{"cell_type":"markdown","source":"# Tokenizing","metadata":{}},{"cell_type":"code","source":"tokenizer = WordLevelTokenizer()\ntokenizer.fit(text)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:02:52.139040Z","iopub.execute_input":"2024-07-24T15:02:52.139315Z","iopub.status.idle":"2024-07-24T15:02:58.707482Z","shell.execute_reply.started":"2024-07-24T15:02:52.139293Z","shell.execute_reply":"2024-07-24T15:02:58.706442Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Building Vocabulary: 100%|██████████| 193460/193460 [00:00<00:00, 723329.91it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"temp = \"Dunlop (lead guitar, banjo, backing vocals) and Neil Primrose (drums, percussion).\"\n\ntokens = tokenizer.tokenize(temp)\nprint(\"Tokens:\", tokens)  # Output: Tokens: [index values representing each word]\n\noriginal_text = tokenizer.detokenize(tokens)\nprint(\"Detokenized text:\", original_text)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:02:58.708873Z","iopub.execute_input":"2024-07-24T15:02:58.709227Z","iopub.status.idle":"2024-07-24T15:02:58.715699Z","shell.execute_reply.started":"2024-07-24T15:02:58.709196Z","shell.execute_reply":"2024-07-24T15:02:58.714649Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Tokens: [135530, 160030, 79788, 112020, 131384, 17664, 131384, 115145, 174245, 185110, 48427, 176269, 32490, 160030, 74821, 131384, 121668, 185110, 58028]\nDetokenized text: dunlop ( lead guitar , banjo , backing vocals ) and neil primrose ( drums , percussion ) .\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab_size = get_vocab_size(text)\nprint(\"Vocabulary Size:\", vocab_size)  ","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:02:58.717134Z","iopub.execute_input":"2024-07-24T15:02:58.717754Z","iopub.status.idle":"2024-07-24T15:03:04.787284Z","shell.execute_reply.started":"2024-07-24T15:02:58.717717Z","shell.execute_reply":"2024-07-24T15:03:04.786365Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Vocabulary Size: 193460\n","output_type":"stream"}]},{"cell_type":"code","source":"data = tokenizer.tokenize(text)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:03:04.788638Z","iopub.execute_input":"2024-07-24T15:03:04.788969Z","iopub.status.idle":"2024-07-24T15:03:12.782586Z","shell.execute_reply.started":"2024-07-24T15:03:04.788942Z","shell.execute_reply":"2024-07-24T15:03:12.781556Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"encoded_data = torch.tensor(data, dtype=torch.long)\nprint(encoded_data.shape, encoded_data.dtype)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:03:12.783854Z","iopub.execute_input":"2024-07-24T15:03:12.784126Z","iopub.status.idle":"2024-07-24T15:03:13.975662Z","shell.execute_reply.started":"2024-07-24T15:03:12.784103Z","shell.execute_reply":"2024-07-24T15:03:13.974765Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"torch.Size([8407280]) torch.int64\n","output_type":"stream"}]},{"cell_type":"code","source":"n = int(0.9*len(encoded_data))\ntrain_data = encoded_data[:n]\nval_data = encoded_data[n:]\n\nprint(f\"Training tokens : {len(train_data)}  --- Validation tokens : {len(val_data)}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:03:13.977000Z","iopub.execute_input":"2024-07-24T15:03:13.977367Z","iopub.status.idle":"2024-07-24T15:03:13.994921Z","shell.execute_reply.started":"2024-07-24T15:03:13.977333Z","shell.execute_reply":"2024-07-24T15:03:13.994134Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Training tokens : 7566552  --- Validation tokens : 840728\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Prepare data for training (given text -> next token)","metadata":{}},{"cell_type":"code","source":"x = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:03:13.996281Z","iopub.execute_input":"2024-07-24T15:03:13.996704Z","iopub.status.idle":"2024-07-24T15:03:14.080604Z","shell.execute_reply.started":"2024-07-24T15:03:13.996666Z","shell.execute_reply":"2024-07-24T15:03:14.079694Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"when input is tensor([14430]) the target: 41068\nwhen input is tensor([14430, 41068]) the target: 158821\nwhen input is tensor([ 14430,  41068, 158821]) the target: 92365\nwhen input is tensor([ 14430,  41068, 158821,  92365]) the target: 173561\nwhen input is tensor([ 14430,  41068, 158821,  92365, 173561]) the target: 98265\nwhen input is tensor([ 14430,  41068, 158821,  92365, 173561,  98265]) the target: 98577\nwhen input is tensor([ 14430,  41068, 158821,  92365, 173561,  98265,  98577]) the target: 156082\nwhen input is tensor([ 14430,  41068, 158821,  92365, 173561,  98265,  98577, 156082]) the target: 50216\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Make Batch","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(1337)\nbatch_size = 4\nblock_size = 8\n\ndef get_batch(split):\n    data = train_data if split == \"train\" else val_data\n    ix = torch.randint(len(encoded_data) - block_size, (batch_size,))\n    x = torch.stack([encoded_data[i:i+block_size] for i in ix])\n    y = torch.stack([encoded_data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint(\"Inputs: \")\nprint(xb)\nprint(\"Targets: \")\nprint(yb)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:03:14.081985Z","iopub.execute_input":"2024-07-24T15:03:14.082434Z","iopub.status.idle":"2024-07-24T15:03:14.110647Z","shell.execute_reply.started":"2024-07-24T15:03:14.082401Z","shell.execute_reply":"2024-07-24T15:03:14.109811Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Inputs: \ntensor([[ 23331, 123498, 144949, 111065, 126481,  23331,  84980, 117665],\n        [ 48427, 156117, 137062, 111467, 104621, 159791, 133702,  36058],\n        [183726,  96889, 180325, 131384,  90178, 185419, 151152,  99941],\n        [ 94369,  56596, 164245,  94369,  94369,  24360, 104088, 168921]])\nTargets: \ntensor([[123498, 144949, 111065, 126481,  23331,  84980, 117665,  98265],\n        [156117, 137062, 111467, 104621, 159791, 133702,  36058,  32030],\n        [ 96889, 180325, 131384,  90178, 185419, 151152,  99941, 146643],\n        [ 56596, 164245,  94369,  94369,  24360, 104088, 168921,  93672]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# GPT Model","metadata":{}},{"cell_type":"code","source":"model = GPTLanguageModel(vocab_size=vocab_size)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:03:14.111752Z","iopub.execute_input":"2024-07-24T15:03:14.112058Z","iopub.status.idle":"2024-07-24T15:03:18.586272Z","shell.execute_reply.started":"2024-07-24T15:03:14.112035Z","shell.execute_reply":"2024-07-24T15:03:18.585379Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"GPTLanguageModel(\n  (token_embeding_table): Embedding(193460, 384)\n  (position_embedding_table): Embedding(193460, 384)\n  (blocks): Sequential(\n    (0): Block(\n      (selfAttention): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-3): 4 x Head(\n            (key): Linear(in_features=384, out_features=96, bias=False)\n            (query): Linear(in_features=384, out_features=96, bias=False)\n            (value): Linear(in_features=384, out_features=96, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ffwd): FeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n    (1): Block(\n      (selfAttention): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-3): 4 x Head(\n            (key): Linear(in_features=384, out_features=96, bias=False)\n            (query): Linear(in_features=384, out_features=96, bias=False)\n            (value): Linear(in_features=384, out_features=96, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ffwd): FeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n    (2): Block(\n      (selfAttention): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-3): 4 x Head(\n            (key): Linear(in_features=384, out_features=96, bias=False)\n            (query): Linear(in_features=384, out_features=96, bias=False)\n            (value): Linear(in_features=384, out_features=96, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ffwd): FeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n    (3): Block(\n      (selfAttention): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-3): 4 x Head(\n            (key): Linear(in_features=384, out_features=96, bias=False)\n            (query): Linear(in_features=384, out_features=96, bias=False)\n            (value): Linear(in_features=384, out_features=96, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ffwd): FeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n  (lm_head): Linear(in_features=384, out_features=193460, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            X, Y = X.to(device), Y.to(device)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:03:18.587570Z","iopub.execute_input":"2024-07-24T15:03:18.587875Z","iopub.status.idle":"2024-07-24T15:03:18.594348Z","shell.execute_reply.started":"2024-07-24T15:03:18.587850Z","shell.execute_reply":"2024-07-24T15:03:18.593484Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    if iter % eval_iters == 0:\n        losses = estimate_loss()\n        print(f\"steps: {iter} train loss: {losses['train']} val loss: {losses['val']}\")\n        \n    xb, yb = get_batch('train')\n    xb = xb.to(device)\n    yb = yb.to(device)\n\n    logits, loss = model.forward(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\nprint(loss.item())","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:10:58.326831Z","iopub.execute_input":"2024-07-24T15:10:58.327897Z","iopub.status.idle":"2024-07-24T16:20:57.485607Z","shell.execute_reply.started":"2024-07-24T15:10:58.327853Z","shell.execute_reply":"2024-07-24T16:20:57.484613Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"steps: 0 train loss: 6.749795913696289 val loss: 6.657464027404785\nsteps: 500 train loss: 6.763338088989258 val loss: 6.7010908126831055\nsteps: 1000 train loss: 6.693780899047852 val loss: 6.704863548278809\nsteps: 1500 train loss: 6.620708465576172 val loss: 6.721517562866211\nsteps: 2000 train loss: 6.659325122833252 val loss: 6.592809200286865\nsteps: 2500 train loss: 6.723973751068115 val loss: 6.611209392547607\nsteps: 3000 train loss: 6.631080627441406 val loss: 6.684095859527588\nsteps: 3500 train loss: 6.685678005218506 val loss: 6.649492263793945\nsteps: 4000 train loss: 6.58201265335083 val loss: 6.659134864807129\nsteps: 4500 train loss: 6.581142425537109 val loss: 6.594021320343018\nsteps: 5000 train loss: 6.6346540451049805 val loss: 6.569756507873535\nsteps: 5500 train loss: 6.61437463760376 val loss: 6.549781322479248\nsteps: 6000 train loss: 6.584932804107666 val loss: 6.589010715484619\nsteps: 6500 train loss: 6.576292991638184 val loss: 6.5551581382751465\nsteps: 7000 train loss: 6.572055816650391 val loss: 6.5106282234191895\nsteps: 7500 train loss: 6.470609664916992 val loss: 6.543970584869385\nsteps: 8000 train loss: 6.569962024688721 val loss: 6.561598777770996\nsteps: 8500 train loss: 6.582768440246582 val loss: 6.589479446411133\nsteps: 9000 train loss: 6.476504802703857 val loss: 6.5918498039245605\nsteps: 9500 train loss: 6.477103233337402 val loss: 6.466765880584717\nsteps: 10000 train loss: 6.4604620933532715 val loss: 6.478074073791504\nsteps: 10500 train loss: 6.536404609680176 val loss: 6.4733195304870605\nsteps: 11000 train loss: 6.448565483093262 val loss: 6.494683265686035\nsteps: 11500 train loss: 6.532707214355469 val loss: 6.46372127532959\nsteps: 12000 train loss: 6.40795373916626 val loss: 6.38895320892334\nsteps: 12500 train loss: 6.449712753295898 val loss: 6.441127300262451\nsteps: 13000 train loss: 6.443113327026367 val loss: 6.485118389129639\nsteps: 13500 train loss: 6.441282272338867 val loss: 6.450594902038574\nsteps: 14000 train loss: 6.444150447845459 val loss: 6.423647880554199\nsteps: 14500 train loss: 6.471843719482422 val loss: 6.483765602111816\nsteps: 15000 train loss: 6.494297981262207 val loss: 6.440552711486816\nsteps: 15500 train loss: 6.4304094314575195 val loss: 6.355261325836182\nsteps: 16000 train loss: 6.486534118652344 val loss: 6.391507625579834\nsteps: 16500 train loss: 6.366787910461426 val loss: 6.385769367218018\nsteps: 17000 train loss: 6.353463649749756 val loss: 6.317083358764648\nsteps: 17500 train loss: 6.363469123840332 val loss: 6.41929817199707\nsteps: 18000 train loss: 6.484414577484131 val loss: 6.519154071807861\nsteps: 18500 train loss: 6.386265277862549 val loss: 6.458481311798096\nsteps: 19000 train loss: 6.430316925048828 val loss: 6.386526584625244\nsteps: 19500 train loss: 6.364755630493164 val loss: 6.382185459136963\nsteps: 20000 train loss: 6.394157409667969 val loss: 6.452213287353516\nsteps: 20500 train loss: 6.4167070388793945 val loss: 6.355414867401123\nsteps: 21000 train loss: 6.375126361846924 val loss: 6.36298131942749\nsteps: 21500 train loss: 6.395633697509766 val loss: 6.374687671661377\nsteps: 22000 train loss: 6.329626560211182 val loss: 6.3673481941223145\nsteps: 22500 train loss: 6.3424482345581055 val loss: 6.3363447189331055\nsteps: 23000 train loss: 6.344343185424805 val loss: 6.439018726348877\nsteps: 23500 train loss: 6.301213264465332 val loss: 6.396917343139648\nsteps: 24000 train loss: 6.407312393188477 val loss: 6.371814250946045\nsteps: 24500 train loss: 6.351243019104004 val loss: 6.297980785369873\nsteps: 25000 train loss: 6.311741828918457 val loss: 6.31165075302124\nsteps: 25500 train loss: 6.3330583572387695 val loss: 6.377051830291748\nsteps: 26000 train loss: 6.354366302490234 val loss: 6.29707145690918\nsteps: 26500 train loss: 6.305370807647705 val loss: 6.21049690246582\nsteps: 27000 train loss: 6.353662014007568 val loss: 6.3131561279296875\nsteps: 27500 train loss: 6.367880344390869 val loss: 6.387839317321777\nsteps: 28000 train loss: 6.453131675720215 val loss: 6.348191738128662\nsteps: 28500 train loss: 6.4012980461120605 val loss: 6.314052104949951\nsteps: 29000 train loss: 6.298930644989014 val loss: 6.311556339263916\nsteps: 29500 train loss: 6.424147605895996 val loss: 6.313190460205078\nsteps: 30000 train loss: 6.392122745513916 val loss: 6.381892681121826\nsteps: 30500 train loss: 6.255820274353027 val loss: 6.227658748626709\nsteps: 31000 train loss: 6.333107948303223 val loss: 6.28794527053833\nsteps: 31500 train loss: 6.312730312347412 val loss: 6.3264851570129395\nsteps: 32000 train loss: 6.335422039031982 val loss: 6.377944469451904\nsteps: 32500 train loss: 6.397254467010498 val loss: 6.333755016326904\nsteps: 33000 train loss: 6.295163154602051 val loss: 6.295571804046631\nsteps: 33500 train loss: 6.3729400634765625 val loss: 6.271579265594482\nsteps: 34000 train loss: 6.299227714538574 val loss: 6.3329949378967285\nsteps: 34500 train loss: 6.302817344665527 val loss: 6.294694900512695\nsteps: 35000 train loss: 6.428348541259766 val loss: 6.323584079742432\nsteps: 35500 train loss: 6.295051097869873 val loss: 6.303483486175537\nsteps: 36000 train loss: 6.268699645996094 val loss: 6.320080280303955\nsteps: 36500 train loss: 6.307673454284668 val loss: 6.331587314605713\nsteps: 37000 train loss: 6.290210723876953 val loss: 6.259321689605713\nsteps: 37500 train loss: 6.332115173339844 val loss: 6.216647624969482\nsteps: 38000 train loss: 6.371319770812988 val loss: 6.383124828338623\nsteps: 38500 train loss: 6.269935131072998 val loss: 6.357711315155029\nsteps: 39000 train loss: 6.217126846313477 val loss: 6.275733947753906\nsteps: 39500 train loss: 6.3996171951293945 val loss: 6.3584885597229\nsteps: 40000 train loss: 6.365100860595703 val loss: 6.339022636413574\nsteps: 40500 train loss: 6.336005210876465 val loss: 6.283034801483154\nsteps: 41000 train loss: 6.29480504989624 val loss: 6.278789043426514\nsteps: 41500 train loss: 6.331070423126221 val loss: 6.333747386932373\nsteps: 42000 train loss: 6.3015666007995605 val loss: 6.374391555786133\nsteps: 42500 train loss: 6.336128234863281 val loss: 6.38713264465332\nsteps: 43000 train loss: 6.387424945831299 val loss: 6.3467326164245605\nsteps: 43500 train loss: 6.316531181335449 val loss: 6.261998653411865\nsteps: 44000 train loss: 6.295803070068359 val loss: 6.259787082672119\nsteps: 44500 train loss: 6.3344645500183105 val loss: 6.292797565460205\nsteps: 45000 train loss: 6.2330217361450195 val loss: 6.254677772521973\nsteps: 45500 train loss: 6.295901298522949 val loss: 6.32390022277832\nsteps: 46000 train loss: 6.356293201446533 val loss: 6.314994812011719\nsteps: 46500 train loss: 6.328488826751709 val loss: 6.283353805541992\nsteps: 47000 train loss: 6.352222442626953 val loss: 6.412013053894043\nsteps: 47500 train loss: 6.339362144470215 val loss: 6.308682441711426\nsteps: 48000 train loss: 6.309599876403809 val loss: 6.371295928955078\nsteps: 48500 train loss: 6.304224967956543 val loss: 6.3998703956604\nsteps: 49000 train loss: 6.282303810119629 val loss: 6.265743255615234\nsteps: 49500 train loss: 6.290656089782715 val loss: 6.319251537322998\nsteps: 50000 train loss: 6.354795455932617 val loss: 6.374962329864502\nsteps: 50500 train loss: 6.32049036026001 val loss: 6.365909099578857\nsteps: 51000 train loss: 6.281839847564697 val loss: 6.304450988769531\nsteps: 51500 train loss: 6.330151081085205 val loss: 6.25786018371582\nsteps: 52000 train loss: 6.340096473693848 val loss: 6.339538097381592\nsteps: 52500 train loss: 6.225673198699951 val loss: 6.274283409118652\nsteps: 53000 train loss: 6.399824142456055 val loss: 6.267776966094971\nsteps: 53500 train loss: 6.296981334686279 val loss: 6.3658366203308105\nsteps: 54000 train loss: 6.248539924621582 val loss: 6.350518703460693\nsteps: 54500 train loss: 6.243374347686768 val loss: 6.33725118637085\nsteps: 55000 train loss: 6.34537410736084 val loss: 6.372681617736816\nsteps: 55500 train loss: 6.270288467407227 val loss: 6.310386657714844\nsteps: 56000 train loss: 6.342227935791016 val loss: 6.3539719581604\nsteps: 56500 train loss: 6.274415016174316 val loss: 6.311081409454346\nsteps: 57000 train loss: 6.3101301193237305 val loss: 6.394266128540039\nsteps: 57500 train loss: 6.344340801239014 val loss: 6.241345405578613\nsteps: 58000 train loss: 6.3221330642700195 val loss: 6.295320510864258\nsteps: 58500 train loss: 6.373641490936279 val loss: 6.405918598175049\nsteps: 59000 train loss: 6.302309513092041 val loss: 6.281240940093994\nsteps: 59500 train loss: 6.307964324951172 val loss: 6.362698554992676\n6.226986885070801\n","output_type":"stream"}]},{"cell_type":"code","source":"model_path = '/kaggle/working/trained_model.pth'\ntorch.save(model.state_dict(), model_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:35:38.901992Z","iopub.execute_input":"2024-07-24T16:35:38.902763Z","iopub.status.idle":"2024-07-24T16:35:40.657991Z","shell.execute_reply.started":"2024-07-24T16:35:38.902718Z","shell.execute_reply":"2024-07-24T16:35:40.657174Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"context = torch.zeros((1,1), dtype=torch.long, device=device)\ngenerated_word = model.generate(context, max_new_token=16)\nseq = \" \"\nfor i in generated_word.tolist():\n    seq = seq + \" \"+tokenizer.detokenize(i)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:33:36.725534Z","iopub.execute_input":"2024-07-24T16:33:36.726280Z","iopub.status.idle":"2024-07-24T16:33:38.333831Z","shell.execute_reply.started":"2024-07-24T16:33:36.726246Z","shell.execute_reply":"2024-07-24T16:33:38.332604Z"},"trusted":true},"execution_count":23,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m----> 2\u001b[0m generated_word \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m generated_word\u001b[38;5;241m.\u001b[39mtolist():\n","File \u001b[0;32m/kaggle/working/large-language-model-creation/models/gpt.py:139\u001b[0m, in \u001b[0;36mGPTLanguageModel.generate\u001b[0;34m(self, index, max_new_token)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, index, max_new_token):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_token):\n\u001b[0;32m--> 139\u001b[0m         logits, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m         logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m    141\u001b[0m         probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/kaggle/working/large-language-model-creation/models/gpt.py:124\u001b[0m, in \u001b[0;36mGPTLanguageModel.forward\u001b[0;34m(self, index, targets)\u001b[0m\n\u001b[1;32m    122\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks(x)\n\u001b[1;32m    123\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[0;32m--> 124\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.90 GiB. GPU 0 has a total capacty of 15.89 GiB of which 4.13 GiB is free. Process 3579 has 11.76 GiB memory in use. Of the allocated memory 10.31 GiB is allocated by PyTorch, and 1.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 5.90 GiB. GPU 0 has a total capacty of 15.89 GiB of which 4.13 GiB is free. Process 3579 has 11.76 GiB memory in use. Of the allocated memory 10.31 GiB is allocated by PyTorch, and 1.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}